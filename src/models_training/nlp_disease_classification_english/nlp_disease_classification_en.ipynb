{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maria\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la carpeta actual\n",
    "current_dir = Path.cwd()\n",
    "DATASETS_LOCATION = os.path.join(current_dir.parent.parent.parent, 'datasets')\n",
    "MODELS_LOCATION = os.path.join(current_dir.parent.parent, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symptoms</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been feeling tired all the time and notic...</td>\n",
       "      <td>Anemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lately, I’ve been out of breath even after lig...</td>\n",
       "      <td>Anemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the past few weeks, I’ve been extremely we...</td>\n",
       "      <td>Anemia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Symptoms Disease\n",
       "0  I've been feeling tired all the time and notic...  Anemia\n",
       "1  Lately, I’ve been out of breath even after lig...  Anemia\n",
       "2  For the past few weeks, I’ve been extremely we...  Anemia"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disease_sp = pd.read_excel(os.path.join(DATASETS_LOCATION, 'disease_nlp_eng.xlsx'))\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df_disease_sp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Disease Spanish antes de undersampling:\")\n",
    "print(df_disease_sp['Enfermedad'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que ya las clases están bien distribuidas. Dividimos el dataset en conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiamo il dataset bilanciato in train (80%) e test (20%)\n",
    "train_df, test_df = train_test_split(df_disease_sp, test_size=0.2, random_state=42, stratify=df_disease_sp['Enfermedad'])\n",
    "\n",
    "# Dividiamo ulteriormente il train in train (80%) e validation (20%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['Enfermedad'])\n",
    "\n",
    "# Verifichiamo che i set siano equilibrati\n",
    "print(\"\\nConjunto de entrenamiento:\")\n",
    "print(train_df['Enfermedad'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de validación:\")\n",
    "print(val_df['Enfermedad'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "print(test_df['Enfermedad'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un codificador de etiquetas para convertir las clases de texto en números\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir las clases de 'Enfermedad' a números\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['Enfermedad'])\n",
    "val_df['label'] = label_encoder.transform(val_df['Enfermedad'])\n",
    "test_df['label'] = label_encoder.transform(test_df['Enfermedad'])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de métricas\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"precision\": precision_score(p.label_ids, preds, average='macro'),\n",
    "        \"recall\": recall_score(p.label_ids, preds, average='macro'),\n",
    "        \"f1\": f1_score(p.label_ids, preds, average='macro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo y el tokenizer\n",
    "model_checkpoint = \"bertin-project/bertin-roberta-base-spanish\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamos los datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['Síntomas'], examples['Enfermedad'], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti i DataFrame in dataset di Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "validation_dataset = Dataset.from_pandas(val_df)\n",
    "# Tokenizamos el dataset\n",
    "train_preprocessed_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "val_preprocessed_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,              # Tasa de aprendizaje: se ha probado 5e-6, 1e-5, 5e-5\n",
    "    per_device_train_batch_size=32,   # Tamaño del batch para entrenamiento: se ha probado 8, 16, 32, 64\n",
    "    per_device_eval_batch_size=32,    # Tamaño del batch para evaluación\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.3,               # Decaimiento de peso: hemos probado 0.01, 0.1, 0.2 e 0.3 e 0.4 y este era el mejor\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_preprocessed_dataset,\n",
    "    eval_dataset=val_preprocessed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics   # Función para calcular las métricas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No modificar esta celda\n",
    "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
    "\n",
    "start = time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end = time()\n",
    "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en el conjunto de prueba\n",
    "\n",
    "# Preprocesamos el conjunto de prueba\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Evaluamos el modelo en el conjunto de prueba\n",
    "results = trainer.evaluate(test_preprocessed_dataset)\n",
    "\n",
    "# Imprimimos las métricas de evaluación\n",
    "print(\"Resultados de la evaluación en el conjunto de prueba:\")\n",
    "print(f\"Exactitud: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precisión: {results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo y el tokenizer\n",
    "save_directory = os.path.join(MODELS_LOCATION, 'disease_classification_spanish_nlp')\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Modelo y tokenizer guardados en el directorio {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos algunas frases de ejemplo para el test que puedan confundir a nuestro modelo\n",
    "sample_sentences = [\n",
    "    {\"Síntomas\": \"Me siento cansado todo el tiempo, como si no tuviera energía para nada.\", \"Enfermedad\": \"Anemia\"},\n",
    "    {\"Síntomas\": \"Mi hijo no crece como debería y siempre se queja de estar débil.\", \"Enfermedad\": \"Talasemia\"},\n",
    "    {\"Síntomas\": \"Tengo dolores en los huesos y me siento agotado incluso después de descansar.\", \"Enfermedad\": \"Talasemia\"},\n",
    "    {\"Síntomas\": \"Tengo mucha sed todo el tiempo y no dejo de ir al baño.\", \"Enfermedad\": \"Diabetes\"},\n",
    "    {\"Síntomas\": \"Tengo un dolor punzante y la pierna está hinchada y roja.\", \"Enfermedad\": \"Trombosis\"},\n",
    "    {\"Síntomas\": \"He bajado de peso sin razón y me siento muy cansado.\", \"Enfermedad\": \"Diabetes\"}\n",
    "]\n",
    "\n",
    "# Convertiamo le frasi di esempio in un DataFrame\n",
    "sample_df = pd.DataFrame(sample_sentences)\n",
    "\n",
    "# Tokenizziamo le frasi di esempio\n",
    "sample_dataset = Dataset.from_pandas(sample_df)\n",
    "sample_preprocessed_dataset = sample_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Effettuiamo le predizioni\n",
    "sample_predictions = trainer.predict(sample_preprocessed_dataset)\n",
    "sample_preds = np.argmax(sample_predictions.predictions, axis=1)\n",
    "sample_df[\"Enfermedad predicha\"] = label_encoder.inverse_transform(sample_preds)\n",
    "\n",
    "# Mostriamo i risultati\n",
    "print(\"Resultados de las predicciones en las frases de ejemplo:\")\n",
    "sample_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
