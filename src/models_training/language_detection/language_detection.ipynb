{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la carpeta actual\n",
    "current_dir = Path.cwd()\n",
    "DATASETS_LOCATION = os.path.join(current_dir.parent.parent.parent, 'datasets')\n",
    "MODELS_LOCATION = os.path.join(current_dir.parent.parent, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_languages = pd.read_csv(os.path.join(DATASETS_LOCATION, 'language_detection.csv'))\n",
    "\n",
    "# Filtramos por idioma espa帽ol e ingl茅s, que son los que nos interesan\n",
    "df_spanish_english = df_all_languages[df_all_languages['Language'].isin(['Spanish', 'English'])]\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "df_spanish_english.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Language distribution antes de undersampling:\n",
      "Language\n",
      "English    1385\n",
      "Spanish     819\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Dataset Language distribution despues de undersampling:\n",
      "Language\n",
      "English    819\n",
      "Spanish    819\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Language distribution antes de undersampling:\")\n",
    "print(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Determinamos la cantidad m铆nima de ejemplos por clase\n",
    "min_count = min(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Submuestreamos para equilibrar las clases\n",
    "df_balanced = pd.concat([\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'English'].sample(n=min_count, random_state=42),\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'Spanish']\n",
    "])\n",
    "\n",
    "# Verificamos el equilibrio\n",
    "print(\"\\n Dataset Language distribution despues de undersampling:\")\n",
    "print(df_balanced['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto de entrenamiento:\n",
      "Language\n",
      "Spanish    524\n",
      "English    524\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Conjunto de validaci贸n:\n",
      "Language\n",
      "Spanish    131\n",
      "English    131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Conjunto de prueba:\n",
      "Language\n",
      "English    164\n",
      "Spanish    164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dividiamo il dataset bilanciato in train (80%) e test (20%)\n",
    "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Language'])\n",
    "\n",
    "# Dividiamo ulteriormente il train in train (80%) e validation (20%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['Language'])\n",
    "\n",
    "# Verifichiamo che i set siano equilibrati\n",
    "print(\"\\nConjunto de entrenamiento:\")\n",
    "print(train_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de validaci贸n:\")\n",
    "print(val_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "print(test_df['Language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>El proyecto tuvo el apoyo econ贸mico de la empr...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In Linnaeus' system, these became the kingdoms...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>It is a powerful tool we are only just beginni...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5163</th>\n",
       "      <td>Por lo tanto es un proceso de inducci贸n del co...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Notably, the results of a Wikimedia Foundation...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Language  label\n",
       "4853  El proyecto tuvo el apoyo econ贸mico de la empr...  Spanish      1\n",
       "177   In Linnaeus' system, these became the kingdoms...  English      0\n",
       "1008  It is a powerful tool we are only just beginni...  English      0\n",
       "5163  Por lo tanto es un proceso de inducci贸n del co...  Spanish      1\n",
       "413   Notably, the results of a Wikimedia Foundation...  English      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un codificador de etiquetas para convertir las clases de texto en n煤meros\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir las clases de 'Language' a n煤meros\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['Language'])\n",
    "val_df['label'] = label_encoder.transform(val_df['Language'])\n",
    "test_df['label'] = label_encoder.transform(test_df['Language'])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la funci贸n de m茅tricas\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"precision\": precision_score(p.label_ids, preds, average='macro'),\n",
    "        \"recall\": recall_score(p.label_ids, preds, average='macro'),\n",
    "        \"f1\": f1_score(p.label_ids, preds, average='macro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo y el tokenizer\n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"  # Usaremos el modelo preentrenado BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamos los datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['Text'], examples['Language'], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b38aaba3d024af9b76214c8e729b994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822ee9819a3948f7990f785aebf8c6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3573ae21947f44c6a41865ab29bd5af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converti i DataFrame in dataset di Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "validation_dataset = Dataset.from_pandas(val_df)\n",
    "# Tokenizamos el dataset\n",
    "train_preprocessed_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "val_preprocessed_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_26144\\2732148159.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,              # Tasa de aprendizaje: se ha probado 5e-6, 1e-5, 5e-5\n",
    "    per_device_train_batch_size=32,   # Tama帽o del batch para entrenamiento: se ha probado 8, 16, 32, 64\n",
    "    per_device_eval_batch_size=32,    # Tama帽o del batch para evaluaci贸n\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.3,               # Decaimiento de peso: hemos probado 0.01, 0.1, 0.2 e 0.3 e 0.4 y este era el mejor\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_preprocessed_dataset,\n",
    "    eval_dataset=val_preprocessed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics   # Funci贸n para calcular las m茅tricas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'Language', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1048\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a7d55b844e48d39c548a49b6699663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2329906c79450cb0d883877bcd4264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19888177514076233, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 22.6454, 'eval_samples_per_second': 11.57, 'eval_steps_per_second': 0.397, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac75b659ba574b7a865dc5b9055f6ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.024797309190034866, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 11.9686, 'eval_samples_per_second': 21.891, 'eval_steps_per_second': 0.752, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003af378ab8d4d6287243449e57a3b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.016829293221235275, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 10.9713, 'eval_samples_per_second': 23.88, 'eval_steps_per_second': 0.82, 'epoch': 3.0}\n",
      "{'train_runtime': 631.7735, 'train_samples_per_second': 4.976, 'train_steps_per_second': 0.157, 'train_loss': 0.2066112287116773, 'epoch': 3.0}\n",
      ">>>>>>>>>>>>> elapsed time: 11m\n"
     ]
    }
   ],
   "source": [
    "# No modificar esta celda\n",
    "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
    "\n",
    "start = time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end = time()\n",
    "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af089ac803544e30b41c0b68633466b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528382226a414d89aff411dac74a94e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la evaluaci贸n en el conjunto de prueba:\n",
      "Exactitud: 1.0000\n",
      "Precisi贸n: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b2d43c344b4328b3ba4207ad7f63b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud calculada manualmente: 1.0000\n",
      "Precisi贸n calculada manualmente: 1.0000\n",
      "Recall calculado manualmente: 1.0000\n",
      "F1-Score calculado manualmente: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci贸n del modelo en el conjunto de prueba\n",
    "\n",
    "# Preprocesamos el conjunto de prueba\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Evaluamos el modelo en el conjunto de prueba\n",
    "results = trainer.evaluate(test_preprocessed_dataset)\n",
    "\n",
    "# Imprimimos las m茅tricas de evaluaci贸n\n",
    "print(\"Resultados de la evaluaci贸n en el conjunto de prueba:\")\n",
    "print(f\"Exactitud: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precisi贸n: {results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['eval_f1']:.4f}\")\n",
    "\n",
    "# Para obtener m谩s detalles sobre las predicciones y las m茅tricas, tambi茅n puedes obtener las predicciones:\n",
    "# Realizamos predicciones en el conjunto de prueba\n",
    "predictions = trainer.predict(test_preprocessed_dataset)\n",
    "\n",
    "# Calculamos las m茅tricas manualmente si lo deseas\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Exactitud\n",
    "accuracy = accuracy_score(y_true, preds)\n",
    "print(f\"Exactitud calculada manualmente: {accuracy:.4f}\")\n",
    "\n",
    "# Precisi贸n, recall y F1-score con el promedio macro\n",
    "precision = precision_score(y_true, preds, average='macro')\n",
    "recall = recall_score(y_true, preds, average='macro')\n",
    "f1 = f1_score(y_true, preds, average='macro')\n",
    "\n",
    "print(f\"Precisi贸n calculada manualmente: {precision:.4f}\")\n",
    "print(f\"Recall calculado manualmente: {recall:.4f}\")\n",
    "print(f\"F1-Score calculado manualmente: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello e tokenizer salvati nella directory c:\\Users\\maria\\Desktop\\universidad\\master\\TFM\\tfm\\src\\models\\languiage_detection\n"
     ]
    }
   ],
   "source": [
    "# Salviamo il modello e il tokenizer\n",
    "save_directory = os.path.join(MODELS_LOCATION, 'languiage_detection')\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Modello e tokenizer salvati nella directory {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27db3b8e36e5490ab79a8e8cebc9394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6afe40bfff44bc884b27867d8b8a8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati delle predizioni sulle frasi di esempio:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Predicted Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you want tortilla for dinner?</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Este es un ejemplo de software developement.</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soy un cloud engineer, trabajo en google</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text Language Predicted Language\n",
       "0              Do you want tortilla for dinner?  English            English\n",
       "1  Este es un ejemplo de software developement.  Spanish            Spanish\n",
       "2      Soy un cloud engineer, trabajo en google  Spanish            Spanish"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definiamo alcune frasi di esempio per il test che possano confondere il modello\n",
    "sample_sentences = [\n",
    "    {\"Text\": \"Do you want tortilla for dinner?\", \"Language\": \"English\"},\n",
    "    {\"Text\": \"Este es un ejemplo de software developement.\", \"Language\": \"Spanish\"},\n",
    "    {\"Text\": \"Soy un cloud engineer, trabajo en google\", \"Language\": \"Spanish\"}\n",
    "]\n",
    "\n",
    "# Convertiamo le frasi di esempio in un DataFrame\n",
    "sample_df = pd.DataFrame(sample_sentences)\n",
    "\n",
    "# Tokenizziamo le frasi di esempio\n",
    "sample_dataset = Dataset.from_pandas(sample_df)\n",
    "sample_preprocessed_dataset = sample_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Effettuiamo le predizioni\n",
    "sample_predictions = trainer.predict(sample_preprocessed_dataset)\n",
    "sample_preds = np.argmax(sample_predictions.predictions, axis=1)\n",
    "sample_df[\"Predicted Language\"] = label_encoder.inverse_transform(sample_preds)\n",
    "\n",
    "# Mostriamo i risultati\n",
    "print(\"Risultati delle predizioni sulle frasi di esempio:\")\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo ri-addestrato un modello gia' capace di fare language classification sui nostri dati che contenevano solo spagnolo e inglese. ci aspettavamo dei risultati molto buoni. dato che questa non e' la parte focale del progetto utilizzeremo questo modello "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
