{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la carpeta actual\n",
    "current_dir = Path.cwd()\n",
    "DATASETS_LOCATION = os.path.join(current_dir.parent.parent, 'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "df_all_languages = pd.read_csv(os.path.join(DATASETS_LOCATION, 'language_detection.csv'))\n",
    "\n",
    "# Filtramos por idioma español e inglés, que son los que nos interesan\n",
    "df_spanish_english = df_all_languages[df_all_languages['Language'].isin(['Spanish', 'English'])]\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "df_spanish_english.head(3)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Language distribution antes de undersampling:\")\n",
    "print(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Determinamos la cantidad mínima de ejemplos por clase\n",
    "min_count = min(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Submuestreamos para equilibrar las clases\n",
    "df_balanced = pd.concat([\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'English'].sample(n=min_count, random_state=42),\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'Spanish']\n",
    "])\n",
    "\n",
    "# Verificamos el equilibrio\n",
    "print(\"\\n Dataset Language distribution despues de undersampling:\")\n",
    "print(df_balanced['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiamo il dataset bilanciato in train (80%) e test (20%)\n",
    "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Language'])\n",
    "\n",
    "# Dividiamo ulteriormente il train in train (80%) e validation (20%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['Language'])\n",
    "\n",
    "# Verifichiamo che i set siano equilibrati\n",
    "print(\"\\nConjunto de entrenamiento:\")\n",
    "print(train_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de validación:\")\n",
    "print(val_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "print(test_df['Language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Language distribution antes de undersampling:\n",
      "Language\n",
      "English    1385\n",
      "Spanish     819\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Dataset Language distribution despues de undersampling:\n",
      "Language\n",
      "English    819\n",
      "Spanish    819\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Language distribution antes de undersampling:\")\n",
    "print(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Determinamos la cantidad mínima de ejemplos por clase\n",
    "min_count = min(df_spanish_english['Language'].value_counts())\n",
    "\n",
    "# Submuestreamos para equilibrar las clases\n",
    "df_balanced = pd.concat([\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'English'].sample(n=min_count, random_state=42),\n",
    "    df_spanish_english[df_spanish_english['Language'] == 'Spanish']\n",
    "])\n",
    "\n",
    "# Verificamos el equilibrio\n",
    "print(\"\\n Dataset Language distribution despues de undersampling:\")\n",
    "print(df_balanced['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto de entrenamiento:\n",
      "Language\n",
      "Spanish    524\n",
      "English    524\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Conjunto de validación:\n",
      "Language\n",
      "Spanish    131\n",
      "English    131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Conjunto de prueba:\n",
      "Language\n",
      "English    164\n",
      "Spanish    164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dividiamo il dataset bilanciato in train (80%) e test (20%)\n",
    "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42, stratify=df_balanced['Language'])\n",
    "\n",
    "# Dividiamo ulteriormente il train in train (80%) e validation (20%)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['Language'])\n",
    "\n",
    "# Verifichiamo che i set siano equilibrati\n",
    "print(\"\\nConjunto de entrenamiento:\")\n",
    "print(train_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de validación:\")\n",
    "print(val_df['Language'].value_counts())\n",
    "\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "print(test_df['Language'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>El proyecto tuvo el apoyo económico de la empr...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In Linnaeus' system, these became the kingdoms...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>It is a powerful tool we are only just beginni...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5163</th>\n",
       "      <td>Por lo tanto es un proceso de inducción del co...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Notably, the results of a Wikimedia Foundation...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Language  label\n",
       "4853  El proyecto tuvo el apoyo económico de la empr...  Spanish      1\n",
       "177   In Linnaeus' system, these became the kingdoms...  English      0\n",
       "1008  It is a powerful tool we are only just beginni...  English      0\n",
       "5163  Por lo tanto es un proceso de inducción del co...  Spanish      1\n",
       "413   Notably, the results of a Wikimedia Foundation...  English      0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> 7dd0aec (commit)
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Crear un codificador de etiquetas para convertir las clases de texto en números\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir las clases de 'Language' a números\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['Language'])\n",
    "val_df['label'] = label_encoder.transform(val_df['Language'])\n",
    "test_df['label'] = label_encoder.transform(test_df['Language'])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función de métricas\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"precision\": precision_score(p.label_ids, preds, average='macro'),\n",
    "        \"recall\": recall_score(p.label_ids, preds, average='macro'),\n",
    "        \"f1\": f1_score(p.label_ids, preds, average='macro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52486caa616c4128919ed853c10a8765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maria\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b86dc762a48079a82f80f97074eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d4dbe7e23349e1bd2d050ac42eba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5212b6ed69804ba0b64d6c5e1b0de538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaab52293d340f4adf8aab07e2e5cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "# Definimos el modelo y el tokenizer\n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"  # Usaremos el modelo preentrenado BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamos los datos\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['Text'], examples['Language'], padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135ad1ec6dc54e778af4f8e8ea711c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3300d19a3e744074ae6faabd86f3c1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87adce649ee14cac86247e5c24b40495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "# Converti i DataFrame in dataset di Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "validation_dataset = Dataset.from_pandas(val_df)\n",
    "# Tokenizamos el dataset\n",
    "train_preprocessed_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "val_preprocessed_dataset = validation_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_7292\\2732148159.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,              # Tasa de aprendizaje: se ha probado 5e-6, 1e-5, 5e-5\n",
    "    per_device_train_batch_size=32,   # Tamaño del batch para entrenamiento: se ha probado 8, 16, 32, 64\n",
    "    per_device_eval_batch_size=32,    # Tamaño del batch para evaluación\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.3,               # Decaimiento de peso: hemos probado 0.01, 0.1, 0.2 e 0.3 e 0.4 y este era el mejor\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_preprocessed_dataset,\n",
    "    eval_dataset=val_preprocessed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics   # Función para calcular las métricas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'Language', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1048\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "train_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa4405628be446fa8650fcd648bcf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a528124e380f4842a1208ce6d39a8c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39639031887054443, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 15.1978, 'eval_samples_per_second': 17.239, 'eval_steps_per_second': 0.592, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7853a8c10cf145e0867847c5653c34d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06483984738588333, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 15.763, 'eval_samples_per_second': 16.621, 'eval_steps_per_second': 0.571, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826bc73976bf4645b5cce460161449d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.042082857340574265, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 15.516, 'eval_samples_per_second': 16.886, 'eval_steps_per_second': 0.58, 'epoch': 3.0}\n",
      "{'train_runtime': 1707.9644, 'train_samples_per_second': 1.841, 'train_steps_per_second': 0.058, 'train_loss': 0.3536457870945786, 'epoch': 3.0}\n",
      ">>>>>>>>>>>>> elapsed time: 28m\n"
     ]
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "# No modificar esta celda\n",
    "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
    "\n",
    "start = time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end = time()\n",
    "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b2e87dd8574aa79a038df9d1cc68b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c76c776edf47d7b4a93f2db7cf4999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la evaluación en el conjunto de prueba:\n",
      "Exactitud: 1.0000\n",
      "Precisión: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b897a8c75c479882fccaed46b3a153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud calculada manualmente: 1.0000\n",
      "Precisión calculada manualmente: 1.0000\n",
      "Recall calculado manualmente: 1.0000\n",
      "F1-Score calculado manualmente: 1.0000\n"
     ]
    }
   ],
>>>>>>> 7dd0aec (commit)
   "source": [
    "# Evaluación del modelo en el conjunto de prueba\n",
    "\n",
    "# Preprocesamos el conjunto de prueba\n",
    "test_preprocessed_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Evaluamos el modelo en el conjunto de prueba\n",
    "results = trainer.evaluate(test_preprocessed_dataset)\n",
    "\n",
    "# Imprimimos las métricas de evaluación\n",
    "print(\"Resultados de la evaluación en el conjunto de prueba:\")\n",
    "print(f\"Exactitud: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precisión: {results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['eval_f1']:.4f}\")\n",
    "\n",
    "# Para obtener más detalles sobre las predicciones y las métricas, también puedes obtener las predicciones:\n",
    "# Realizamos predicciones en el conjunto de prueba\n",
    "predictions = trainer.predict(test_preprocessed_dataset)\n",
    "\n",
    "# Calculamos las métricas manualmente si lo deseas\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Exactitud\n",
    "accuracy = accuracy_score(y_true, preds)\n",
    "print(f\"Exactitud calculada manualmente: {accuracy:.4f}\")\n",
    "\n",
    "# Precisión, recall y F1-score con el promedio macro\n",
    "precision = precision_score(y_true, preds, average='macro')\n",
    "recall = recall_score(y_true, preds, average='macro')\n",
    "f1 = f1_score(y_true, preds, average='macro')\n",
    "\n",
    "print(f\"Precisión calculada manualmente: {precision:.4f}\")\n",
    "print(f\"Recall calculado manualmente: {recall:.4f}\")\n",
    "print(f\"F1-Score calculado manualmente: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
